<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AlexNet-2</title>
      <link href="/2022/12/15/AlexNet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"/>
      <url>/2022/12/15/AlexNet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="AlexNet研读"><a href="#AlexNet研读" class="headerlink" title="AlexNet研读"></a>AlexNet研读</h1><p><a href="https://dl.acm.org/doi/pdf/10.1145/3065386">ImageNet Classification with Deep Convolutional Neural Networks</a>,【<a href="https://blog.csdn.net/weixin_36670529/article/details/99976379">中文翻译</a>】</p><h2 id="网络优点"><a href="#网络优点" class="headerlink" title="网络优点"></a>网络优点</h2><p>###<strong>ReLU非线性单元</strong></p><p>AlexNet使用的神经元激活函数是ReLU激活函数 f(x)&#x3D;max(0,x) ，相比于饱和非线性函数如Sigmoid和tanh函数，不饱和非线性函数如ReLU在梯度下降时具有<strong>更快的收敛速度</strong>，更快地学习，在大型网络中训练大量数据具有非常好的效果。作者通过在CIFAR-10数据集上做的实验证明了此论点。如下图，实线表示使用ReLUs的CNN，虚线表示使用tanh函数的CNN，可以看出使用ReLUs的CNN能更快地把训练错误率降低到25%（迭代次数比tanh的CNN快约5倍）。</p><p>ReLU是本文作者Hinton在2010年提出来改善RBM性能的，把ReLU引入深度CNN中，使得ReLU成为以后深度网络普遍使用的非线性函数，从而在这个领域替代经典的sigmoid、tanh函数。ReLU有三个好处：</p><ol><li><strong>简单的max计算，大大减少了计算量</strong>，可以提高训练速度；</li><li>梯度在ReLU中是直接传递的，鉴于深度网络的梯度衰减现象，<strong>ReLU可以保持梯度，减缓梯度衰减</strong>的趋势；</li><li><strong>bp过程中没有了梯度换算的操作</strong>，加快了训练。</li></ol><h3 id="跨GPU并行化操作"><a href="#跨GPU并行化操作" class="headerlink" title="跨GPU并行化操作"></a><strong>跨GPU并行化操作</strong></h3><p>一个GTX580的内存只有3GB，有限的内存限制了可以在GPU上训练的最大网络。目前的GPU很适合于跨GPU并行化操作，故作者把网络一分为二，分配到2个GPU上，通过并行计算来解决，不用通过主机的缓存，当前GPU相互间可以很好进行读写操作。</p><p>这里作者有一个小技巧，GPU相互“沟通”：例如，网络中layer-3的filter把layer-2的所有特征图作为输入，而其它卷积层，只从同一个GPU内的上一层特征图作为输入。为什么layer-3把layer-2的全部特征图作为输入而其它层却不这样，这个作者并没有解释理论依据，通过交叉验证实验得来的。最终的结构有点类似Cresian提出的多列卷积网络，但是本文的网络不是完全独立的，这种方式可以提高1.2%的准确率。</p><h3 id="局部响应归一化"><a href="#局部响应归一化" class="headerlink" title="局部响应归一化"></a>局部响应归一化</h3><p>LRN（Local Response Normalization）层是用来做归一化的。ReLU函数不需要归一化来防止饱和现象，即对于很大的输入x，ReLU仍然可以有效的学习，但是作者发现即使这样，对数据进行局部归一化对于学习来说还是有帮助的，可以增强模型的泛化能力。</p><p>局部响应归一化公式：</p><p><img src="https://pic3.zhimg.com/80/v2-3c5c3702c0ca1d00a6af49ed306f4d0a_1440w.webp" alt="img"></p><p>具体方式：</p><p>选取临近的n个特征图，在特征图的同一个空间位置（x,y）依次平方，然后求和，在乘以α，在加上 k。这个局部归一化方式与论文“What is the best multi-stage architecture for Object Recognition”中的局部归一化方法不同：本文的归一化只是多个特征图同一个位置上的归一化，属于特征图之间的局部归一化（属于纵向归一化），作者命名为亮度归一化；“what……”论文中在特征图之间基础上还有同一特征图内邻域位置像素的归一化（横向、纵向归一化结合）；“what……”归一化方法计算复杂，但是没有本文中 α、k、n 等参数，本文通过交叉验证来确定这三个参数；此外，本文的<strong>归一化方法没有减去均值</strong>，感觉是因为ReLU只对正值部分产生学习，如果减去均值会丢失掉很多信息。</p><p>简言之，LRN是“What is the best multi-stage architecture for Object Recognition”论文使用的方法，本文简化改进了该归一化方法，将<strong>模型错误率降低1.2%左右</strong>。</p><h3 id="重叠池化"><a href="#重叠池化" class="headerlink" title="重叠池化"></a><strong>重叠池化</strong></h3><p>正常池化是<strong>步长 &#x3D; 窗口尺寸</strong>如步长s&#x3D;2、窗口z&#x3D;2，重叠池化是指<strong>步长＜窗口尺寸</strong>如步长s&#x3D;2、窗口z&#x3D;3。</p><p>Pooling层一般用于降维，将一个 k × k 的区域内取平均或取最大值，作为这一个小区域内的特征，传递到下一层。传统的Pooling层是不重叠的，而本论文提出使Pooling层重叠可以降低错误率，而且对防止过拟合有一定的效果。</p><p>个人理解，使Pooling层重叠，可以<strong>减少信息的损失</strong>，所以<strong>错误率会下降</strong>。但是对防止过拟合的效果，文章也只是说slightly，目测<strong>意义不大。</strong></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h3><p>Dropout是Hinton在2012年提出以改善深度网络泛化能力的。文中采用Dropout技术，在每次训练网络时，每个神经元都会以0.5的概率“丢失”，丢失的神经元不参与前向传播和反向传播，但是神经元的权值在每次训练时都是共享的。这个技术降低了神经元之间的联合适应性，从而学习更具鲁棒性的特征。在测试阶段，神经网络会使用所有的神经元，但每个神经元的输出会乘以0.5。Dropout技术用在前两个全连接层中，<strong>会减少过拟合</strong></p><p>缺点：就是会使最终网络收敛所需要的<strong>迭代次数翻倍</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> AlexNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlexNet-1</title>
      <link href="/2022/12/14/AlexNet%E7%BB%93%E6%9E%84%E7%90%86%E8%AE%BA/"/>
      <url>/2022/12/14/AlexNet%E7%BB%93%E6%9E%84%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="AlexNet网络结构详解"><a href="#AlexNet网络结构详解" class="headerlink" title="AlexNet网络结构详解"></a>AlexNet网络结构详解</h1><p>AlexNet 是2012年 ISLVRC ( ImageNet Large Scale Visual Recognition Challenge)竞赛的冠军网络，分类准确率由传统的70%+提升到80%+。它是由Hinton和他的学生Alex Krizhevsky设计的。 也是在那年之后，深度学习开始迅速发展。</p><p>注：作者使用了两块GPU，所以网络模型变成上下两块</p><p><img src="https://s2.loli.net/2022/12/16/A2CNEHGXlaUwKDY.png"><br>使用Dropout的方式在网络正向传播过程中随机失活一部分神经元，以减少过拟合Conv1</p><p><img src="https://s2.loli.net/2022/12/16/u2UnJ97s4WaX6oj.png" alt="在这里插入图片描述"></p><p><img src="https://s2.loli.net/2022/12/16/EChYiBVGXSRczwM.png" alt="在这里插入图片描述"></p><p><img src="https://s2.loli.net/2022/12/16/IVN9LnhORWS2uMt.png" alt="在这里插入图片描述"></p><h2 id="结构分析"><a href="#结构分析" class="headerlink" title="结构分析"></a>结构分析</h2><h3 id="Conv1"><a href="#Conv1" class="headerlink" title="Conv1"></a>Conv1</h3><p><strong>注意：原作者实验时用了两块GPU并行计算，上下两组图的结构是一样的。</strong></p><ul><li>输入：input_size &#x3D; [224, 224, 3]</li><li>卷积层：<ul><li>kernels &#x3D; 48 * 2 &#x3D; 96 组卷积核</li><li>kernel_size &#x3D; 11</li><li>padding &#x3D; [1, 2] （左，上围加1列圈0，右下围加2列圈0）</li><li>stride &#x3D; 4</li></ul></li><li>输出：output_size &#x3D; [55, 55, 96]<br><img src="https://s2.loli.net/2022/12/16/LkfhXqJ6SpEWBtb.png"></li></ul><h3 id="Maxpool1"><a href="#Maxpool1" class="headerlink" title="Maxpool1"></a>Maxpool1</h3><p><img src="https://s2.loli.net/2022/12/16/U58BtidnLCIF1GN.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [55, 55, 96]</li><li>池化层：（只改变尺寸，不改变深度channel）<ul><li>kernel_size &#x3D; 3</li><li>padding &#x3D; 0</li><li>stride &#x3D; 2</li></ul></li><li>输出：output_size &#x3D; [27, 27, 96]</li></ul><p>经 Maxpool1 后的输出层尺寸为：</p><p><img src="https://s2.loli.net/2022/12/16/V3d8sCLn6SMTp9D.png"></p><h3 id="Conv2"><a href="#Conv2" class="headerlink" title="Conv2"></a>Conv2</h3><p><img src="https://s2.loli.net/2022/12/16/kRoAEqgCz17r36K.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [27, 27, 96]</li><li>卷积层：<ul><li>kernels &#x3D; 128 * 2 &#x3D; 256 组卷积核</li><li>kernel_size &#x3D; 5</li><li>padding &#x3D; [2, 2]</li><li>stride &#x3D; 1</li></ul></li><li>输出：output_size &#x3D; [27, 27, 256]</li></ul><p>经 Conv2 卷积后的输出层尺寸为：<br><img src="https://s2.loli.net/2022/12/16/V3d8sCLn6SMTp9D.png"></p><h3 id="Maxpool2"><a href="#Maxpool2" class="headerlink" title="Maxpool2"></a>Maxpool2</h3><p><img src="https://s2.loli.net/2022/12/16/JlIaUDjTzdLxymW.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [27, 27, 256]</li><li>池化层：（只改变尺寸，不改变深度channel）<ul><li>kernel_size &#x3D; 3</li><li>padding &#x3D; 0</li><li>stride &#x3D; 2</li></ul></li><li>输出：output_size &#x3D; [13, 13, 256]</li></ul><p>经 Maxpool2 后的输出层尺寸为：<br><img src="https://s2.loli.net/2022/12/16/k4LnWYOri7bvKsF.png"></p><h3 id="Conv3"><a href="#Conv3" class="headerlink" title="Conv3"></a>Conv3</h3><p><img src="https://s2.loli.net/2022/12/16/zn87fvVEL6CUjQw.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [13, 13, 256]</li><li>卷积层：<ul><li>kernels &#x3D; 192* 2 &#x3D; 384 组卷积核</li><li>kernel_size &#x3D; 3</li><li>padding &#x3D; [1, 1]</li><li>stride &#x3D; 1</li></ul></li><li>输出：output_size &#x3D; [13, 13, 384]</li></ul><p><img src="https://s2.loli.net/2022/12/16/dKcpH9Y4qD6bvgs.png"></p><h3 id="Conv4"><a href="#Conv4" class="headerlink" title="Conv4"></a>Conv4</h3><p><img src="https://s2.loli.net/2022/12/16/iuDHn3KVcNlkQPg.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [13, 13, 384]</li><li>卷积层：<ul><li>kernels &#x3D; 192* 2 &#x3D; 384 组卷积核</li><li>kernel_size &#x3D; 3</li><li>padding &#x3D; [1, 1]</li><li>stride &#x3D; 1</li></ul></li><li>输出：output_size &#x3D; [13, 13, 384]</li></ul><p><img src="https://s2.loli.net/2022/12/16/mH47a9upRSGCZy3.png"></p><h3 id="Conv5"><a href="#Conv5" class="headerlink" title="Conv5"></a>Conv5</h3><p><img src="https://s2.loli.net/2022/12/16/6yTadbEBt7K4AF3.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [13, 13, 384]</li><li>卷积层：<ul><li>kernels &#x3D; 128* 2 &#x3D; 256 组卷积核</li><li>kernel_size &#x3D; 3</li><li>padding &#x3D; [1, 1]</li><li>stride &#x3D; 1</li></ul></li><li>输出：output_size &#x3D; [13, 13, 256]</li></ul><p><img src="https://s2.loli.net/2022/12/16/wM3TYJk2X6iBupz.png"></p><h3 id="Maxpool3"><a href="#Maxpool3" class="headerlink" title="Maxpool3"></a>Maxpool3</h3><p><img src="https://s2.loli.net/2022/12/16/f6EI3LTg4lNkXWo.png" alt="在这里插入图片描述"></p><ul><li>输入：input_size &#x3D; [13, 13, 256]</li><li>池化层：（只改变尺寸，不改变深度channel）<ul><li>kernel_size &#x3D; 3</li><li>padding &#x3D; 0</li><li>stride &#x3D; 2</li></ul></li><li>输出：output_size &#x3D; [6, 6, 256]</li></ul><p><img src="https://s2.loli.net/2022/12/16/lm9GxnbNzdjUiLW.png"></p><h3 id="FC1、FC2、FC3"><a href="#FC1、FC2、FC3" class="headerlink" title="FC1、FC2、FC3"></a>FC1、FC2、FC3</h3><p>Maxpool3 → (6<em>6</em>256) → FC1 → 2048 → FC2 → 2048 → FC3 → 1000<br><strong>最终的1000可以根据数据集的类别数进行修改。</strong></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://s2.loli.net/2022/12/16/CtqvVzZaSipXolM.png" alt="在这里插入图片描述"><br>分析可以发现，除 Conv1 外，AlexNet 的其余卷积层都是在改变特征矩阵的深度，而池化层则只改变（减小)其尺寸。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> AlexNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeNet-2</title>
      <link href="/2022/12/12/Pytorch_Demo/"/>
      <url>/2022/12/12/Pytorch_Demo/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch官网入门Demo——实现一个图像分类器"><a href="#Pytorch官网入门Demo——实现一个图像分类器" class="headerlink" title="Pytorch官网入门Demo——实现一个图像分类器"></a>Pytorch官网入门Demo——实现一个图像分类器</h1><p>参考：</p><ol><li><a href="https://www.bilibili.com/video/BV187411T7Ye">哔哩哔哩：pytorch官方demo(Lenet)</a></li><li><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#">pytorch官网demo</a>（<a href="https://pytorch.apachecn.org/#/docs/1.4/4">中文版戳这里</a>）</li><li><a href="https://blog.csdn.net/qq_37541097/article/details/102926037">pytorch中的卷积操作详解</a></li><li><a href="https://blog.csdn.net/m0_37867091/article/details/107136477">Fan的CSDN笔记</a></li></ol><h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><p>Model 模型构建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):                     <span class="comment"># 继承于nn.Module这个父类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):                     <span class="comment"># 初始化网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()       <span class="comment"># 多继承需用到super函数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>)    <span class="comment">#构建卷积层1，深度3，卷积核个数为16，核大小为5*5 输出深度为卷积核个数</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)     <span class="comment">#构建池化层（下采样层）1，核大小为2*2，步长为2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>)   <span class="comment">#构建卷积层2，深度16，卷积核个数为32，核大小为5*5</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)     <span class="comment">#构建池化层（下采样层）2，核大小为2*2，步长为2</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)   <span class="comment">#构建池全连接层1，输入为32*5*5，输出为120</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)       <span class="comment">#构建池全连接层2，输入为120，输出为84</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)        <span class="comment">#构建全连接层3，输入为84，输出为10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):            <span class="comment"># 正向传播过程</span></span><br><span class="line">        x = F.relu(self.conv1(x))    <span class="comment"># input(3, 32, 32) output(16, 28, 28)</span></span><br><span class="line">        x = self.pool1(x)            <span class="comment"># output(16, 14, 14)</span></span><br><span class="line">        x = F.relu(self.conv2(x))    <span class="comment"># output(32, 10, 10)</span></span><br><span class="line">        x = self.pool2(x)            <span class="comment"># output(32, 5, 5)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)       <span class="comment"># output(32*5*5) 展开为一维数据</span></span><br><span class="line">        x = F.relu(self.fc1(x))      <span class="comment"># output(120)</span></span><br><span class="line">        x = F.relu(self.fc2(x))      <span class="comment"># output(84)</span></span><br><span class="line">        x = self.fc3(x)              <span class="comment"># output(10)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># import torch</span></span><br><span class="line"><span class="comment"># input1 = torch.rand([32,3,32,32])</span></span><br><span class="line"><span class="comment"># model = LeNet()</span></span><br><span class="line"><span class="comment"># print(model) #输出LeNet神经网络架构</span></span><br><span class="line"><span class="comment"># output = model(input1)</span></span><br></pre></td></tr></table></figure><p>Train训练（下载cifar-10官方训练集和测试集）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    transform = transforms.Compose(</span><br><span class="line">        [transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"><span class="comment"># ToTensor将文件高*宽*深度转换诶 深度*高*宽 Normalize标准化 使用均值和标准差 标准化图像</span></span><br><span class="line">    <span class="comment"># 50000张训练图片</span></span><br><span class="line">    <span class="comment"># 第一次使用时要将download设置为True才会自动去下载数据集</span></span><br><span class="line">    train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                             download=<span class="literal">False</span>, transform=transform) <span class="comment">#此行download改为True为下载数据集合，transform为预处理</span></span><br><span class="line"><span class="comment">#torchvision.datasets. 可以用于查看pytorch官方数据集</span></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">36</span>,</span><br><span class="line">                                               shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)<span class="comment">#shuffle为是否打乱数据集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 10000张验证图片</span></span><br><span class="line">    <span class="comment"># 第一次使用时要将download设置为True才会自动去下载数据集</span></span><br><span class="line">    val_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                           download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    val_loader = torch.utils.data.DataLoader(val_set, batch_size=<span class="number">5000</span>,</span><br><span class="line">                                             shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)<span class="comment">#预览时可以把batchsize调低用于查看</span></span><br><span class="line">    val_data_iter = <span class="built_in">iter</span>(val_loader)<span class="comment">#转换为可迭代的迭代器</span></span><br><span class="line">    val_image, val_label = <span class="built_in">next</span>(val_data_iter)<span class="comment">#获取一批图像并赋值</span></span><br><span class="line"></span><br><span class="line">    classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)<span class="comment">#导入标签</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 输出图像的函数</span></span><br><span class="line">    <span class="comment"># def imshow(img):</span></span><br><span class="line">    <span class="comment">#     img = img / 2 + 0.5  # unnormalize反标准化</span></span><br><span class="line">    <span class="comment">#     npimg = img.numpy() #转化为numpy格式</span></span><br><span class="line">    <span class="comment">#     plt.imshow(np.transpose(npimg, (1, 2, 0)))</span></span><br><span class="line">    <span class="comment">#     # 反标准化为原始格式高，款，深度</span></span><br><span class="line">    <span class="comment">#     plt.show()</span></span><br><span class="line">    <span class="comment"># # 打印图片标签</span></span><br><span class="line">    <span class="comment"># print(&#x27; &#x27;.join(&#x27;%5s&#x27; % classes[val_label[j]] for j in range(4)))</span></span><br><span class="line">    <span class="comment"># # 显示图片</span></span><br><span class="line">    <span class="comment"># imshow(torchvision.utils.make_grid(val_image))</span></span><br><span class="line"></span><br><span class="line">    net = LeNet()</span><br><span class="line">    loss_function = nn.CrossEntropyLoss()<span class="comment">#定义损失函数  包含softmax函数</span></span><br><span class="line">    optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)<span class="comment">#Adam优化器；net.parameters()将网络中所有可训练参数进行训练；lr(learning rate)学习率</span></span><br><span class="line">    <span class="comment">#以下为训练过程</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):  <span class="comment"># loop over the dataset multiple times；epoch训练集迭代次数</span></span><br><span class="line"></span><br><span class="line">        running_loss = <span class="number">0.0</span><span class="comment">#累加损失</span></span><br><span class="line">        <span class="comment">#以下迭代损失</span></span><br><span class="line">        <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">            <span class="comment"># get the inputs; data is a list of [inputs, labels] 获取输入 enumerate返回data和data的步数</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># zero the parameter gradients 历史损失梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># forward + backward + optimize</span></span><br><span class="line">            outputs = net(inputs)</span><br><span class="line">            loss = loss_function(outputs, labels)<span class="comment">#损失计算</span></span><br><span class="line">            loss.backward()<span class="comment">#反向传播</span></span><br><span class="line">            optimizer.step()<span class="comment">#利用优化器进行参数更新</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># print statistics</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">499</span>:    <span class="comment"># print every 500 mini-batches 每隔500步打印一次</span></span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():<span class="comment">#减少计算损失梯度</span></span><br><span class="line">                    outputs = net(val_image)  <span class="comment"># [batch, 10]</span></span><br><span class="line">                    predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]<span class="comment">#在输出10个节点找到最大值 获得标签</span></span><br><span class="line">                    accuracy = torch.eq(predict_y, val_label).<span class="built_in">sum</span>().item() / val_label.size(<span class="number">0</span>)<span class="comment">#输出预测对了多少个样本，获得测试准确率</span></span><br><span class="line"></span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">                          (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">500</span>, accuracy))<span class="comment">#打印训练轮数，训练多少步，累加误差，获得训练误差</span></span><br><span class="line">                    running_loss = <span class="number">0.0</span><span class="comment">#清零</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span><span class="comment">#保存参数和模型</span></span><br><span class="line">    torch.save(net.state_dict(), save_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Files already downloaded and verified</span><br><span class="line">[1,   500] train_loss: 1.747  test_accuracy: 0.459</span><br><span class="line">[1,  1000] train_loss: 1.445  test_accuracy: 0.510</span><br><span class="line">[2,   500] train_loss: 1.230  test_accuracy: 0.575</span><br><span class="line">[2,  1000] train_loss: 1.173  test_accuracy: 0.601</span><br><span class="line">[3,   500] train_loss: 1.034  test_accuracy: 0.612</span><br><span class="line">[3,  1000] train_loss: 1.035  test_accuracy: 0.629</span><br><span class="line">[4,   500] train_loss: 0.941  test_accuracy: 0.645</span><br><span class="line">[4,  1000] train_loss: 0.928  test_accuracy: 0.649</span><br><span class="line">[5,   500] train_loss: 0.846  test_accuracy: 0.666</span><br><span class="line">[5,  1000] train_loss: 0.866  test_accuracy: 0.670</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure><p>预测模块</p><p>输入图片</p><p><img src="https://s2.loli.net/2022/12/10/hloJP1QY3FDdcaK.jpg" alt="totest"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    transform = transforms.Compose(</span><br><span class="line">        [transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),<span class="comment">#缩放图片</span></span><br><span class="line">         transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">    classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    net = LeNet()</span><br><span class="line">    net.load_state_dict(torch.load(<span class="string">&#x27;Lenet.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    im = Image.<span class="built_in">open</span>(<span class="string">&#x27;totest.jpg&#x27;</span>)</span><br><span class="line">    im = transform(im)  <span class="comment"># [C, H, W]</span></span><br><span class="line">    im = torch.unsqueeze(im, dim=<span class="number">0</span>)  <span class="comment"># [N, C, H, W]增加维度 batch</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = net(im)</span><br><span class="line">        predict = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].numpy()</span><br><span class="line">    <span class="built_in">print</span>(classes[<span class="built_in">int</span>(predict)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出结果</p><pre><code>plane</code></pre><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h3 id="Demo流程"><a href="#Demo流程" class="headerlink" title="Demo流程"></a>Demo流程</h3><ol><li>model.py ——定义LeNet网络模型</li><li>train.py ——加载数据集并训练，训练集计算loss，测试集计算accuracy，保存训练好的网络参数</li><li>predict.py——得到训练好的网络参数后，用自己找的图像进行分类测试</li></ol><h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):                     <span class="comment"># 继承于nn.Module这个父类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):                     <span class="comment"># 初始化网络结构</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()       <span class="comment"># 多继承需用到super函数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>)    <span class="comment">#构建卷积层1，深度3，卷积核个数为16，核大小为5*5 输出深度为卷积核个数</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)     <span class="comment">#构建池化层（下采样层）1，核大小为2*2，步长为2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>)   <span class="comment">#构建卷积层2，深度16，卷积核个数为32，核大小为5*5</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)     <span class="comment">#构建池化层（下采样层）2，核大小为2*2，步长为2</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)   <span class="comment">#构建池全连接层1，输入为32*5*5，输出为120</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)       <span class="comment">#构建池全连接层2，输入为120，输出为84</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)        <span class="comment">#构建全连接层3，输入为84，输出为10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):            <span class="comment"># 正向传播过程</span></span><br><span class="line">        x = F.relu(self.conv1(x))    <span class="comment"># input(3, 32, 32) output(16, 28, 28)</span></span><br><span class="line">        x = self.pool1(x)            <span class="comment"># output(16, 14, 14)</span></span><br><span class="line">        x = F.relu(self.conv2(x))    <span class="comment"># output(32, 10, 10)</span></span><br><span class="line">        x = self.pool2(x)            <span class="comment"># output(32, 5, 5)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)       <span class="comment"># output(32*5*5) 展开为一维数据</span></span><br><span class="line">        x = F.relu(self.fc1(x))      <span class="comment"># output(120)</span></span><br><span class="line">        x = F.relu(self.fc2(x))      <span class="comment"># output(84)</span></span><br><span class="line">        x = self.fc3(x)              <span class="comment"># output(10)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>：</p><ul><li>pytorch中的卷积、池化、输入输出层中参数的含义与位置，可配合下图一起食用：</li></ul><p><img src="https://s2.loli.net/2022/12/10/vbmuFMxjCrGXWLO.png"></p><h4 id="卷积-Conv2d"><a href="#卷积-Conv2d" class="headerlink" title="卷积 Conv2d"></a>卷积 Conv2d</h4><p>我们常用的卷积（Conv2d）在pytorch中对应的函数是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>)</span><br></pre></td></tr></table></figure><p>一般使用时关注以下几个参数即可：</p><ul><li><strong>in_channels</strong>：输入特征矩阵的深度。如输入一张RGB彩色图像，那in_channels&#x3D;3</li><li><strong>out_channels</strong>：输入特征矩阵的深度。也等于卷积核的个数，使用n个卷积核输出的特征矩阵深度就是n</li><li><strong>kernel_size</strong>：卷积核的尺寸。可以是int类型，如3 代表卷积核的height&#x3D;width&#x3D;3，也可以是tuple类型如(3, 5)代表卷积核的height&#x3D;3，width&#x3D;5</li><li><strong>stride</strong>：卷积核的步长。默认为1，和kernel_size一样输入可以是int型，也可以是tuple类型</li><li><strong>padding</strong>：补零操作，默认为0。可以为int型如1即补一圈0，如果输入为tuple型如(2, 1) 代表在上下补2行，左右补1列。</li></ul><blockquote><p>附上pytorch官网上的公式：<img src="https://s2.loli.net/2022/12/10/Zp84TjmrdkEex3B.png"></p></blockquote><ul><li><img src="https://s2.loli.net/2022/12/10/MGygxue5QlJskb9.png"></li></ul><p>注：当通过<strong>N &#x3D; (W − F + 2P ) &#x2F; S + 1</strong>计算式得到的输出尺寸非整数时，会通过删除多余的行和列来保证卷积的输出尺寸为整数。</p><h4 id="池化-MaxPool2d"><a href="#池化-MaxPool2d" class="headerlink" title="池化 MaxPool2d"></a>池化 MaxPool2d</h4><p>最大池化（MaxPool2d）在 pytorch 中对应的函数是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MaxPool2d(kernel_size, stride) <span class="comment">#核大小;步长</span></span><br></pre></td></tr></table></figure><h4 id="Tensor的展平：view"><a href="#Tensor的展平：view" class="headerlink" title="Tensor的展平：view()"></a>Tensor的展平：view()</h4><p>注意到，在经过第二个池化层后，数据还是一个三维的Tensor (32, 5, 5)，需要先经过展平后(32*5*5)再传到全连接层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = self.pool2(x)            <span class="comment"># output(32, 5, 5)</span></span><br><span class="line">x = x.view(-<span class="number">1</span>, <span class="number">32</span>*<span class="number">5</span>*<span class="number">5</span>)       <span class="comment"># output(32*5*5)</span></span><br><span class="line">x = F.relu(self.fc1(x))      <span class="comment"># output(120)</span></span><br></pre></td></tr></table></figure><h4 id="全连接-Linear"><a href="#全连接-Linear" class="headerlink" title="全连接 Linear"></a>全连接 Linear</h4><p>全连接（ Linear）在 pytorch 中对应的函数是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Linear(in_features, out_features, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/12/10/zENetyB4dPjvinO.png"></p><h3 id="Train-py"><a href="#Train-py" class="headerlink" title="Train.py"></a>Train.py</h3><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><p>导入包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><p>####数据预处理</p><p>对输入的图像数据做预处理，即由shape (H x W x C) in the range [0, 255] → shape (C x H x W) in the range [0.0, 1.0]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br></pre></td></tr></table></figure><h4 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h4><p>利用<code>torchvision.datasets</code>函数可以在线导入pytorch中的数据集，包含一些常见的数据集如MNIST等<br><img src="https://s2.loli.net/2022/12/10/3qdHunSgXamG5YJ.png"><br>此demo用的是CIFAR10数据集，也是一个很经典的图像分类数据集，由 Hinton 的学生 Alex Krizhevsky 和 Ilya Sutskever 整理的一个用于识别普适物体的小型数据集，一共包含 10 个类别的 RGB 彩色图片。<br><img src="https://s2.loli.net/2022/12/10/Ws63JIl1NgoafDn.png"></p><h4 id="导入、加载-训练集"><a href="#导入、加载-训练集" class="headerlink" title="导入、加载 训练集"></a>导入、加载 训练集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入50000张训练图片</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>,  <span class="comment"># 数据集存放目录</span></span><br><span class="line"> train=<span class="literal">True</span>, <span class="comment"># 表示是数据集中的训练集</span></span><br><span class="line">                                        download=<span class="literal">True</span>,   <span class="comment"># 第一次运行时为True，下载数据集，下载完成后改为False</span></span><br><span class="line">                                        transform=transform) <span class="comment"># 预处理过程</span></span><br><span class="line"><span class="comment"># 加载训练集，实际过程需要分批次（batch）训练                                        </span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set,   <span class="comment"># 导入的训练集</span></span><br><span class="line">   batch_size=<span class="number">50</span>, <span class="comment"># 每批训练的样本数</span></span><br><span class="line">                                          shuffle=<span class="literal">False</span>,  <span class="comment"># 是否打乱训练集</span></span><br><span class="line">                                          num_workers=<span class="number">0</span>)  <span class="comment"># 使用线程数，在windows下设置为0</span></span><br></pre></td></tr></table></figure><h4 id="导入、加载-测试集"><a href="#导入、加载-测试集" class="headerlink" title="导入、加载 测试集"></a>导入、加载 测试集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入10000张测试图片</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, </span><br><span class="line">train=<span class="literal">False</span>,<span class="comment"># 表示是数据集中的测试集</span></span><br><span class="line">                                        download=<span class="literal">False</span>,transform=transform)</span><br><span class="line"><span class="comment"># 加载测试集</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set, </span><br><span class="line">  batch_size=<span class="number">10000</span>, <span class="comment"># 每批用于验证的样本数</span></span><br><span class="line">  shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 获取测试集中的图像和标签，用于accuracy计算</span></span><br><span class="line">test_data_iter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">test_image, test_label = test_data_iter.<span class="built_in">next</span>()</span><br><span class="line"><span class="number">1234567891011</span></span><br></pre></td></tr></table></figure><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><table><thead><tr><th>名词</th><th>定义</th></tr></thead><tbody><tr><td>epoch</td><td>对训练集的全部数据进行一次完整的训练，称为 一次 epoch</td></tr><tr><td>batch</td><td>由于硬件算力有限，实际训练时将训练集分成多个批次训练，每批数据的大小为 batch_size</td></tr><tr><td>iteration 或 step</td><td>对一个batch的数据训练的过程称为 一个 iteration 或 step</td></tr></tbody></table><p>以本demo为例，训练集一共有50000个样本，batch_size&#x3D;50，那么完整的训练一次样本：iteration或step&#x3D;1000，epoch&#x3D;1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">net = LeNet()  <span class="comment"># 定义训练的网络模型</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss() <span class="comment"># 定义损失函数为交叉熵损失函数 </span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># 定义优化器（训练参数，学习率）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):  <span class="comment"># 一个epoch即对整个训练集进行一次训练</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    time_start = time.perf_counter()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):   <span class="comment"># 遍历训练集，step从0开始计算</span></span><br><span class="line">        inputs, labels = data <span class="comment"># 获取训练集的图像和标签</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清除历史梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)    <span class="comment"># 正向传播</span></span><br><span class="line">        loss = loss_function(outputs, labels) <span class="comment"># 计算损失</span></span><br><span class="line">        loss.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()   <span class="comment"># 优化器更新参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印耗时、损失、准确率等数据</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># print every 1000 mini-batches，每1000步打印一次</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 在以下步骤中（验证过程中）不用计算每个节点的损失梯度，防止内存占用</span></span><br><span class="line">                outputs = net(test_image)  <span class="comment"># 测试集传入网络（test_batch_size=10000），output维度为[10000,10]</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 以output中值最大位置对应的索引（标签）作为预测输出</span></span><br><span class="line">                accuracy = (predict_y == test_label).<span class="built_in">sum</span>().item() / test_label.size(<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %  <span class="comment"># 打印epoch，step，loss，accuracy</span></span><br><span class="line">                      (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">500</span>, accuracy))</span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;%f s&#x27;</span> % (time.perf_counter() - time_start))        <span class="comment"># 打印耗时</span></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存训练得到的参数</span></span><br><span class="line">save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), save_path)</span><br></pre></td></tr></table></figure><p>打印信息如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.537</span>  test_accuracy: <span class="number">0.541</span></span><br><span class="line"><span class="number">35.345407</span> s</span><br><span class="line">[<span class="number">2</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.198</span>  test_accuracy: <span class="number">0.605</span></span><br><span class="line"><span class="number">40.532376</span> s</span><br><span class="line">[<span class="number">3</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.048</span>  test_accuracy: <span class="number">0.641</span></span><br><span class="line"><span class="number">44.144097</span> s</span><br><span class="line">[<span class="number">4</span>,  <span class="number">1000</span>] train_loss: <span class="number">0.954</span>  test_accuracy: <span class="number">0.647</span></span><br><span class="line"><span class="number">41.313228</span> s</span><br><span class="line">[<span class="number">5</span>,  <span class="number">1000</span>] train_loss: <span class="number">0.882</span>  test_accuracy: <span class="number">0.662</span></span><br><span class="line"><span class="number">41.860646</span> s</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure><h4 id="使用GPU-x2F-CPU训练"><a href="#使用GPU-x2F-CPU训练" class="headerlink" title="使用GPU&#x2F;CPU训练"></a>使用GPU&#x2F;CPU训练</h4><p>使用下面语句可以在有GPU时使用GPU，无GPU时使用CPU进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br></pre></td></tr></table></figure><p>也可以直接指定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="comment"># device = torch.device(&quot;cpu&quot;)</span></span><br></pre></td></tr></table></figure><p>对应的，需要用<code>to()</code>函数来将Tensor在CPU和GPU之间相互移动，分配到指定的device中计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">net = LeNet()</span><br><span class="line">net.to(device) <span class="comment"># 将网络分配到指定的device中</span></span><br><span class="line">loss_function = nn.CrossEntropyLoss() </span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>): </span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    time_start = time.perf_counter()<span class="comment">#添加计时器</span></span><br><span class="line">    <span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs.to(device))  <span class="comment"># 将inputs分配到指定的device中</span></span><br><span class="line">        loss = loss_function(outputs, labels.to(device))  <span class="comment"># 将labels分配到指定的device中</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">999</span>:    </span><br><span class="line">            <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">                outputs = net(test_image.to(device)) <span class="comment"># 将test_image分配到指定的device中</span></span><br><span class="line">                predict_y = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">                accuracy = (predict_y == test_label.to(device)).<span class="built_in">sum</span>().item() / test_label.size(<span class="number">0</span>) <span class="comment"># 将test_label分配到指定的device中</span></span><br><span class="line"></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span> %</span><br><span class="line">                      (epoch + <span class="number">1</span>, step + <span class="number">1</span>, running_loss / <span class="number">1000</span>, accuracy))</span><br><span class="line"></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;%f s&#x27;</span> % (time.perf_counter() - time_start))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line">save_path = <span class="string">&#x27;./Lenet.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), save_path)</span><br></pre></td></tr></table></figure><p>打印信息如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cuda</span><br><span class="line">[<span class="number">1</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.569</span>  test_accuracy: <span class="number">0.527</span></span><br><span class="line"><span class="number">18.727597</span> s</span><br><span class="line">[<span class="number">2</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.235</span>  test_accuracy: <span class="number">0.595</span></span><br><span class="line"><span class="number">17.367685</span> s</span><br><span class="line">[<span class="number">3</span>,  <span class="number">1000</span>] train_loss: <span class="number">1.076</span>  test_accuracy: <span class="number">0.623</span></span><br><span class="line"><span class="number">17.654908</span> s</span><br><span class="line">[<span class="number">4</span>,  <span class="number">1000</span>] train_loss: <span class="number">0.984</span>  test_accuracy: <span class="number">0.639</span></span><br><span class="line"><span class="number">17.861825</span> s</span><br><span class="line">[<span class="number">5</span>,  <span class="number">1000</span>] train_loss: <span class="number">0.917</span>  test_accuracy: <span class="number">0.649</span></span><br><span class="line"><span class="number">17.733115</span> s</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure><p>可以看到，用GPU训练时，速度提升明显，耗时缩小。</p><hr><h3 id="Predict-py"><a href="#Predict-py" class="headerlink" title="Predict.py"></a>Predict.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)), <span class="comment"># 首先需resize成跟训练集图像一样的大小</span></span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入要测试的图像（自己找的，不在数据集中），放在源文件目录下</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;horse.jpg&#x27;</span>)</span><br><span class="line">im = transform(im)  <span class="comment"># [C, H, W]</span></span><br><span class="line">im = torch.unsqueeze(im, dim=<span class="number">0</span>)  <span class="comment"># 对数据增加一个新维度，因为tensor的参数是[batch, channel, height, width] </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化网络，加载训练好的模型参数</span></span><br><span class="line">net = LeNet()</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;Lenet.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = net(im)</span><br><span class="line">    predict = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line"><span class="built_in">print</span>(classes[<span class="built_in">int</span>(predict)])</span><br></pre></td></tr></table></figure><p>输出即为预测的标签。</p><p>其实预测结果也可以用 <strong>softmax</strong> 表示，输出10个概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = net(im)</span><br><span class="line">    predict = torch.softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(predict)</span><br></pre></td></tr></table></figure><p>输出结果中最大概率值对应的索引即为 预测标签 的索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2.2782e-06</span>, <span class="number">2.1008e-07</span>, <span class="number">1.0098e-04</span>, <span class="number">9.5135e-05</span>, <span class="number">9.3220e-04</span>, <span class="number">2.1398e-04</span>,</span><br><span class="line">         <span class="number">3.2954e-08</span>, <span class="number">9.9865e-01</span>, <span class="number">2.8895e-08</span>, <span class="number">2.8820e-07</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> LeNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch的安装指南</title>
      <link href="/2022/12/10/How%20to%20install%20Pytorch/"/>
      <url>/2022/12/10/How%20to%20install%20Pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch的安装指南"><a href="#Pytorch的安装指南" class="headerlink" title="Pytorch的安装指南"></a>Pytorch的安装指南</h1><h2 id="视频教程"><a href="#视频教程" class="headerlink" title="视频教程"></a>视频教程</h2><p><a href="https://www.bilibili.com/video/BV1ov41137Z8">环境配置</a></p><p>PS：嫌麻烦anaconda在base环境就行，不需要再建立一个新环境，以防萌新环境混乱。</p><h2 id="一堆官网"><a href="#一堆官网" class="headerlink" title="一堆官网"></a>一堆官网</h2><p>CUDA：<a href="https://developer.nvidia.com/zh-cn/cuda-toolkit">https://developer.nvidia.com/zh-cn/cuda-toolkit</a></p><p>Cudnn：<a href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p>Pytorch：<a href="https://pytorch.org/">https://pytorch.org/</a></p><h3 id="个人版本："><a href="#个人版本：" class="headerlink" title="个人版本："></a>个人版本：</h3><p>CUDA：</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://developer.nvidia.com/cuda-11-7-1-download-archive</span><br></pre></td></tr></table></figure><p>Cudnn：</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://developer.nvidia.com/compute/cudnn/secure/8.6.0/local_installers/11.8/cudnn-windows-x86_64-8.6.0.163_cuda11-archive.zip</span><br></pre></td></tr></table></figure><p>Pytorch：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=<span class="number">11.7</span> -c pytorch -c nvidia</span><br></pre></td></tr></table></figure><p>前两个下载后一个在base环境安装</p><h2 id="一点提示"><a href="#一点提示" class="headerlink" title="一点提示"></a>一点提示</h2><h3 id="在pycharm中terminal直接启用anaconda-base环境的方法"><a href="#在pycharm中terminal直接启用anaconda-base环境的方法" class="headerlink" title="在pycharm中terminal直接启用anaconda-base环境的方法"></a>在pycharm中terminal直接启用anaconda-base环境的方法</h3><ol><li><p>找到该文件右击属性<img src="https://s2.loli.net/2022/12/10/4pC58E2IL9xqHwU.png" style="zoom:80%;" /></p></li><li><p>复制目标路径“cmd”开始所有内容</p><img src="https://s2.loli.net/2022/12/10/21sHhJmaxO9b3CS.png" style="zoom: 80%;" /></li><li><p>settings-tools-terminal-Shell path粘贴你复制的内容 点击OK 重启pycharm<img src="https://s2.loli.net/2022/12/10/m3EGIKWzU59AMb7.png"></p><h3 id="Anaconda中conda更新"><a href="#Anaconda中conda更新" class="headerlink" title="Anaconda中conda更新"></a>Anaconda中conda更新</h3><p>我们用<code>conda install xxx</code>来安装包时，经常会遇到如下问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">failed <span class="keyword">with</span> initial frozen solve. Retrying <span class="keyword">with</span> flexible solve.</span><br><span class="line">Solving environment: failed <span class="keyword">with</span> repodata <span class="keyword">from</span> current_repodata.json, will retry <span class="keyword">with</span> <span class="built_in">next</span> repodata source.</span><br><span class="line"><span class="number">12</span></span><br></pre></td></tr></table></figure><p>这其实是conda的环境不适配，我们需要将对环境进行更新，通过如下几个步骤：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">conda -V</span><br><span class="line"><span class="comment"># 更新conda环境</span></span><br><span class="line">conda update -n base conda</span><br><span class="line"><span class="comment"># 更新conda的所有包</span></span><br><span class="line">conda update --<span class="built_in">all</span></span><br><span class="line"><span class="number">123456</span></span><br></pre></td></tr></table></figure><p>做完这些之后，再用<code>conda install xxx</code>就不会有上面的问题了。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 学习资源 </tag>
            
            <tag> 安装指南 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeNet-1</title>
      <link href="/2022/12/10/LeNet%E7%BB%93%E6%9E%84%E5%8F%8A%E7%A0%94%E8%AF%BB/"/>
      <url>/2022/12/10/LeNet%E7%BB%93%E6%9E%84%E5%8F%8A%E7%A0%94%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Lenet结构及研读"><a href="#Lenet结构及研读" class="headerlink" title="Lenet结构及研读"></a>Lenet结构及研读</h1><p>论文原文下载 <a href="https://link.zhihu.com/?target=http://lushuangning.oss-cn-beijing.aliyuncs.com/CNN%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25B3%25BB%25E5%2588%2597/Gradient-Based_Learning_Applied_to_Document_Recognition.pdf">《Gradient-Based Learning Applied to Document Recognition》</a></p><p><img src="https://s2.loli.net/2022/12/15/z9TWXaBcl6ewj2p.png"></p><p><img src="https://s2.loli.net/2022/12/15/4oq52dXg8zbmFlI.png"></p><h2 id="论文研读以及理解"><a href="#论文研读以及理解" class="headerlink" title="论文研读以及理解"></a>论文研读以及理解</h2><p>使用梯度下降算法的多层网络能够从大量样本中学习到复杂、高维、非线性的映射关系，这使得它们能够用于图像识别任务。在传统的模式识别模型中，手工设计的特征提取器用于从输入数据中提取相关信息并且消除不相关的变量。然后用一个可训练的分类器将结果特征向量分类成对应类别。在该方案中，一个标准的、全连接的多层网络用于当作分类器。一个更有意思的模式是特征提取器能够进行自我学习。在该字符识别任务中，<strong>原始输入的图像即可用于特征提取及分类（标准大小的图像）。虽然可以通过普通的全连接前馈网络成功完成字符识别等任务，但在部分方面仍存在问题。</strong></p><p>第一，<strong>图像比较大</strong>，通常包含几百个像素(pixels)。第一层包含上百个隐藏神经元的全连接层，会包含成千上万的权重。如此大量的参数提高了系统的识别能力，但也导致了需要大量的训练集。并且，存储如此大量的权重超出了当前的硬件承载能力。但是，对于图像或者语音应用而言，缺乏结构性的网络最大不足在于不具备平移、形变扭曲的不变性。在输入到固定大小的网络之前，字符图像或者其他的2D、1D信号必须经过大小的标准化和数据的归一化。不幸的是，没有一种预处理是完美的：手写数字一般在字符层面进行规范化，会导致每个字符的大小，倾斜，位置发生形变，外加上书写风格的差异，会导致输入对象中特征位置的显著变化。原则上，足够大小的全连接网络能够对这些变化具有鲁棒性。可是，这样的任务训练会导致多个具有相似权重模式的神经元位于输入的不同位置，便于检测输入中出现的任何不同特征。需要大量的训练样例来学习这些权重设置从而能够覆盖可能的变量空间。在下文所讨论的卷积网络中，平移不变性能够通过跨空间的权重复制（即权重共享）自动实现。</p><p>第二，<strong>全连接架构的一个不足在于输入的拓扑结构被完全忽略。</strong>输入变量可以以任何（固定）顺序呈现，而不会影响训练结果。与之相对的是，图像（或者语音的时间序列表示）具有健壮的2D局部结构：空间相邻的像素具有高度相关性。局部相关性对于提取局部特征来说具有巨大优势，因为相邻像素的权重可以分类为几类（比如：边，角等）。卷积神经网络通过限制感受野的隐藏神经元为局部大小从而强制提取局部特征。</p><h3 id="Convolutional-Networks-卷积网络"><a href="#Convolutional-Networks-卷积网络" class="headerlink" title="Convolutional Networks 卷积网络"></a><strong>Convolutional Networks 卷积网络</strong></h3><p>卷积网络通过三种架构思想（<strong>局部感受野(local receptive fields)<strong>、</strong>权重共享(shared weights)<strong>、下采样(sub-sampling)）来保证平移、尺度、和形变的不变性。图2中所展示的LeNet-5就是典型的用于字符识别的卷积网络。原始的字符输入图像是经过大小规范化和数据归一化的。局部感受野的连接神经元的理念可追溯到60年代早期的感知机理论，并且几乎与Hubel和Wiesel在猫的视觉系统中发现局部敏感、定向选择性神经元同时发生。局部连接在视觉学习的神经元模型中广泛使用。通过局部感受野，神经元能够提取初级的视觉特征，比如边缘、定点、角点（或者像语音频谱图相似的其他信号特征）。这些特征在接下来的层中能够用于提取高级特征。正如之前所言，输入的平移或者形变会导致特征位置的显著变化。此外，图像的局部特征检测器还可作用于整张图像。这种技术实现可通过强制整张图像不同地方的局部感受野拥有相同的权重向量（即权值共享）。通过这些神经元进行特征提取的输出结果称为特征图（feature map）。特征图的每一神经元都是通过前一层不同位置经过相同计算得来的。一个完整的卷积层是通过几个特征图联合组成的（通过不同的权重向量），因此可以在每个局部位置形成多重特征。图2中展现一个LeNet输入的完整的样本。首层拥有6张由隐藏神经元提取的特征图。特征图的每个神经元计算来自输入中5x5大小总共25个输入计算，这称之为神经元的感受野。每个神经元</strong>通过25个输入乘以25个可训练变量加上一个偏置</strong>得来。特征图中的连续单元的感受野以前一层中相应的连续单元为中心，所以相邻单元的感受野是重叠的。举个例子，在LeNet-5中的首个隐藏层，水平连续单元的感受野重叠5行4列。正如之前所提，特征图中的每个神经元共享相同的25个权重和一个偏置，从而使得输入图像的所有相似的局部特征。在同一层中的其他特征图使用不同的权重和偏置集合，因此能够提取不同类型的局部特征。在LeNet-5中，每一个输入位置，6张特征图都由同一位置的6种神经元组合进行提取。<strong>特征图的一系列实现通过感受野大小扫描输入图片的每一个位置，然后将结果保存在特征图的相同位置。这种操作等同于卷积，外接一个额外的偏置和压缩方法，因此命名为卷积网络(convolutional network)。</strong>卷积核是特征图中神经元所使用的权重结合。卷积层的一个有趣的特征是如果输入图像发生平移，特征图也会发生同样的平移，否则特征图图保持不变。这个特性是CNN对位移和形变保持鲁棒的基础。</p><p><img src="https://s2.loli.net/2022/12/15/kJN2YufDiH8mnPG.png"></p><p>一旦一个特征被提取，它的其他局部就变得不重要了。相对于其他特征的相关位置才是更相关的。举个例子，我们知道左上角区域输入图像包含一个水平线段的端点，右上角区域包括一个角点，下方垂直区域包含一个端点，我们就能得出输入图像为7。这些特征位置的精确对识别没什么帮助，反而不利于不同字符的识别。在特征图中降低精确位置信息的简单方式是通过<strong>降低空间分辨率(spatial resolution)。</strong>这可通过<strong>下采样层(sub-sampling layers)<strong>来达到目标。</strong>下采样层通过局部平均来降低特征图的分辨率</strong>，并且降低输出对平移、形变的敏感度。LeNet-5的第二个隐藏层即为下采样层。这层包含6张特征图，每一张特征图都对应前一层的每张特征图。每个神经元的感受野大小为2x2，每个神经元通过4个输入取平均，然后<strong>乘以一个可训练的参数外加一个偏置</strong>，最后通过一个<strong>sigmoid函数进行激活</strong>。连续单元具有不重叠的感受野。最后，经过<strong>下采样的特征图含有前一层特征图的一半大小的行和列</strong>。一个可训练的参数和偏置影响着最后的sigmoid非线性。如果系数过小，下采样层神经元相当于对输入做了模糊处理，相当于线性。如果系数较大，根据偏置的值下采样层可看成是“或”或者“与”操作。卷积层和下采样层的交替衔接，就形成了一种“金字塔”架构：在每一层，特征图的分辨率逐渐降低，而数量逐渐增加。在图2中的第三层隐藏层的每个神经元的计算来自于上一层的多个特征图的相关神经元计算。卷积和下采样结合的灵感来源于Hubel和Wiesel的“简单”和“复杂”细胞概念，虽然那个时候没有像反向传播算法一样的全局监督学习过程。下采样层结合多个特征图的丰富表达可以<strong>大大提高网络对几何变换的不变性</strong>。</p><p>自从所有权重可通过反向传播算法学习以后，卷积网络能够被视为可自我学习的特征提取器。权重共享的技术能够大大减少参数的使用量，并且该技术降低了机器的“能力”，同时该技术还减小了测试误差和训练误差之间的差距。图2中的网络包含340908个连接，但通过权值共享后，只需要60000个可训练参数。</p><p>固定大小的卷积网络已经适用于各种应用，包括手写识别任务、机打字符识别、在线手写识别以及人脸识别等。**在单个时间维度进行权值共享的固定大小的卷积网络称之为延时神经网络(TDNNs)**，TDNNs已经用于场景识别（没有下采样），语音字符识别（有下采样），独立的手写体字符识别以及手势验证。</p><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a><strong>LeNet-5</strong></h3><p>LeNet-5模型主要是针对灰度设计的，由Yann LeCun教授于1998年提出来的，它是第一个成功应用于数字识别问题的卷积神经网络。在MNIST数据中，它的准确率达到大约99.2%。典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是 <strong>CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer</strong>，即 y ^ \hat{y}y^​。下图所示的是一个数字识别的LeNet-5的模型结构：<br><img src="https://s2.loli.net/2022/12/15/HBzFcgRyKNUVI7Y.png"></p><p>该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。</p><p><img src="https://s2.loli.net/2022/12/15/rlFvBmdcMtqzIVn.png"></p><p>这个网络虽然很小，但是它包含了<a href="http://cuijiahua.com/blog/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>的基本模块：卷积层，池化层，<a href="https://so.csdn.net/so/search?q=%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82&spm=1001.2101.3001.7020">全连接层</a>。是其他深度学习模型的基础， 这里我们对LeNet-5进行深入分析。同时，通过实例分析，加深对与卷积层和池化层的理解。</p><p><img src="https://s2.loli.net/2022/12/15/fzksIaO4Vv2Dm75.png"></p><p>​    LeNet-5共<strong>有7层</strong>，不包含输入，每层都包含可训练参数；每个层有<strong>多个Feature Map</strong>，每个FeatureMap通过一种卷积滤波器提取输入的一种特征，然后每个FeatureMap有<strong>多个神经元。</strong></p><p><strong>各层参数详解：</strong></p><h4 id="1、INPUT层-输入层"><a href="#1、INPUT层-输入层" class="headerlink" title="1、INPUT层-输入层"></a>1、INPUT层-输入层</h4><p>​    首先是数据 INPUT 层，输入图像的尺寸统一归一化为32*32。</p><p>​    <strong>注意：本层不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。</strong></p><h4 id="2、C1层-卷积层"><a href="#2、C1层-卷积层" class="headerlink" title="2、C1层-卷积层"></a>2、C1层-卷积层</h4><p>   输入图片：32*32</p><p>   卷积核大小：5*5</p><p>   卷积核种类：6</p><p>   输出featuremap大小：28*28 （32-5+1）&#x3D;28</p><p>   神经元数量：28<em>28</em>6</p><p>   可训练参数：（5<em>5+1) * 6（每个滤波器5</em>5&#x3D;25个unit参数和一个bias参数，一共6个滤波器）</p><p>   连接数：（5<em>5+1）</em>6<em>28</em>28&#x3D;122304</p><p>   <strong>详细说明：</strong>对输入图像进行第一次卷积运算（使用 6 个大小为 5<em>5 的卷积核），得到6个C1特征图（6个大小为28</em>28的 feature maps, 32-5+1&#x3D;28）。我们再来看看需要多少个参数，卷积核的大小为5<em>5，总共就有6</em>（5<em>5+1）&#x3D;156个参数，其中+1是表示一个核有一个bias。对于卷积层C1，C1内的每个像素都与输入图像中的5</em>5个像素和1个bias有连接，所以总共有156<em>28</em>28&#x3D;122304个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。</p><h4 id="3、S2层-池化层（下采样层）"><a href="#3、S2层-池化层（下采样层）" class="headerlink" title="3、S2层-池化层（下采样层）"></a>3、S2层-池化层（下采样层）</h4><p>   输入：28*28</p><p>   采样区域：2*2</p><p>   采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid</p><p>   采样种类：6</p><p>   输出featureMap大小：14*14（28&#x2F;2）</p><p>   神经元数量：14<em>14</em>6</p><p>   可训练参数：2*6（和的权+偏置）</p><p>   连接数：（2<em>2+1）</em>6<em>14</em>14</p><p>   S2中每个特征图的大小是C1中特征图大小的1&#x2F;4。</p><p>​    <strong>详细说明：</strong>第一次卷积之后紧接着就是池化运算，使用 2<em>2核 进行池化，于是得到了S2，6个14</em>14的 特征图（28&#x2F;2&#x3D;14）。S2这个pooling层是对C1中的2*2区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。于是每个池化核有两个训练参数，所以共有2x6&#x3D;12个训练参数，但是有5x14x14x6&#x3D;5880个连接。</p><h4 id="4、C3层-卷积层"><a href="#4、C3层-卷积层" class="headerlink" title="4、C3层-卷积层"></a>4、C3层-卷积层</h4><p>​    输入：S2中所有6个或者几个特征map组合</p><p>   卷积核大小：5*5</p><p>   卷积核种类：16</p><p>   输出featureMap大小：10*10 (14-5+1)&#x3D;10</p><p>   C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合。</p><p>​    存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。则：可训练参数：6*(3<em>5</em>5+1)+6*(4<em>5</em>5+1)+3*(4<em>5</em>5+1)+1*(6<em>5</em>5+1)&#x3D;1516</p><p>​    连接数：10<em>10</em>1516&#x3D;151600</p><p>​    <strong>详细说明：</strong>第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5<em>5. 我们知道S2 有6个 14</em>14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下：</p><p><img src="https://s2.loli.net/2022/12/15/bzQLhDZu7PrTqMe.png" alt="image-20221215224359251"></p><p>​    C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为5<em>5，所以总共有6</em>(3<em>5</em>5+1)+6*(4<em>5</em>5+1)+3*(4<em>5</em>5+1)+1*(6<em>5</em>5+1)&#x3D;1516个参数。而图像大小为10*10，所以共有151600个连接。</p><p><img src="https://s2.loli.net/2022/12/15/hjOm5ZatkoErS39.png" alt="image-20221215224609718"></p><p>​    C3与S2中前3个图相连的卷积结构如下图所示：</p><p><img src="https://s2.loli.net/2022/12/15/DsCLyKd1uFVgUqt.png"></p><p>​    上图对应的参数为 3<em>5</em>5+1，一共进行6次卷积得到6个特征图，所以有6<em>（3</em>5*5+1）参数。 为什么采用上述这样的组合了？论文中说有两个原因：1）减少参数，2）这种不对称的组合连接的方式有利于提取多种组合特征。</p><h4 id="5、S4层-池化层（下采样层）"><a href="#5、S4层-池化层（下采样层）" class="headerlink" title="5、S4层-池化层（下采样层）"></a>5、S4层-池化层（下采样层）</h4><p>​    输入：10*10</p><p>​    采样区域：2*2</p><p>   采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid</p><p>   采样种类：16</p><p>   输出featureMap大小：5*5（10&#x2F;2）</p><p>   神经元数量：5<em>5</em>16&#x3D;400</p><p>   可训练参数：2*16&#x3D;32（和的权+偏置）</p><p>   连接数：16<em>（2</em>2+1）<em>5</em>5&#x3D;2000</p><p>   S4中每个特征图的大小是C3中特征图大小的1&#x2F;4</p><p>   <strong>详细说明：</strong>S4是pooling层，窗口大小仍然是2*2，共计16个feature map，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。这一层有2x16共32个训练参数，5x5x5x16&#x3D;2000个连接。连接的方式与S2层类似。</p><h4 id="6、C5层-卷积层"><a href="#6、C5层-卷积层" class="headerlink" title="6、C5层-卷积层"></a>6、C5层-卷积层</h4><p>   输入：S4层的全部16个单元特征map（与s4全相连）</p><p>   卷积核大小：5*5</p><p>   卷积核种类：120</p><p>   输出featureMap大小：1*1（5-5+1）</p><p>   可训练参数&#x2F;连接：120<em>（16</em>5*5+1）&#x3D;48120</p><p>   <strong>详细说明：</strong>C5层是一个卷积层。由于S4层的16个图的大小为5x5，与卷积核的大小相同，所以卷积后形成的图的大小为1x1。这里形成120个卷积结果。每个都与上一层的16个图相连。所以共有(5x5x16+1)x120 &#x3D; 48120个参数，同样有48120个连接。C5层的网络结构如下：</p><p><img src="https://s2.loli.net/2022/12/15/ue49pVs68dxlCkh.png"></p><h4 id="7、F6层-全连接层"><a href="#7、F6层-全连接层" class="headerlink" title="7、F6层-全连接层"></a>7、F6层-全连接层</h4><p>   输入：c5 120维向量</p><p>   计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。</p><p>   可训练参数:84*(120+1)&#x3D;10164</p><p>   <strong>详细说明：</strong>6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，-1表示白色，1表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84&#x3D;10164。ASCII编码图如下：</p><p><img src="https://s2.loli.net/2022/12/15/JiD9HlmRLqKzGF6.png"></p><p>   F6层的连接方式如下：</p><p><img src="https://s2.loli.net/2022/12/15/EZHMmayxz2QgwcV.png"></p><h4 id="8、Output层-全连接层"><a href="#8、Output层-全连接层" class="headerlink" title="8、Output层-全连接层"></a>8、Output层-全连接层</h4><p>​    Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：</p><p><img src="https://s2.loli.net/2022/12/15/HZL3t5vycbSVKfd.png"></p><p>   上式w_ij 的值由i的比特图编码确定，i从0到9，j取值从0到7*12-1。RBF输出的值越接近于0，则越接近于i，即越接近于i的ASCII编码图，表示当前网络输入的识别结果是字符i。该层有84x10&#x3D;840个参数和连接。</p><p><img src="https://s2.loli.net/2022/12/15/pdVS2koYELIah57.png"></p><p>上图是LeNet-5识别数字3的过程。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>LeNet-5是一种用于手写体字符识别的非常高效的卷积神经网络。</li><li>卷积神经网络能够很好的利用图像的结构信息。</li><li>卷积层的参数较少，这也是由卷积层的主要特性即<strong>局部连接</strong>和<strong>共享权重</strong>所决定。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> LeNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贪吃蛇大作战分析</title>
      <link href="/2022/12/09/%E8%B4%AA%E5%90%83%E8%9B%87%E5%A4%A7%E4%BD%9C%E6%88%98%E5%88%86%E6%9E%90/"/>
      <url>/2022/12/09/%E8%B4%AA%E5%90%83%E8%9B%87%E5%A4%A7%E4%BD%9C%E6%88%98%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="与对象玩游戏的思考-1"><a href="#与对象玩游戏的思考-1" class="headerlink" title="与对象玩游戏的思考-1"></a>与对象玩游戏的思考-1</h1><img src="https://s2.loli.net/2022/12/10/UxC3Bh7DOsmAtMK.png" alt="贪吃蛇大作战LOGO" style="zoom:33%;" /><p>贪吃蛇大作战是一款对象爱玩的游戏，我也经常陪她一起玩，一开始觉得这个游戏些许幼稚，但一段时间发先此游戏确实有些独特之处，我仅站在我的角度对此款游戏进行分析。</p><h2 id="游戏操作"><a href="#游戏操作" class="headerlink" title="游戏操作"></a>游戏操作</h2><p>作为一款IO型游戏（.io域名网页移植型游戏）它必定轻量化且易上手。左侧摇杆进行方向控制，右侧控制加速按钮和道具&#x2F;技能按钮。玩家通过操控贪吃蛇食取地图上的粒子和大粒子以及道具，延长自身长度。通过灵活操作让别的贪吃蛇撞到自身身体部分达成击杀。对象作为一个讨厌复杂操作的游戏玩家很容易掌握，其上手门槛极低，老少咸宜。</p><h2 id="游戏模式"><a href="#游戏模式" class="headerlink" title="游戏模式"></a>游戏模式</h2><p>实际上分为两类：</p><ul><li>人机模式（无尽模式及其衍生）</li><li>PvP模式（团战模式，击杀模式）</li></ul><p>前者对应了需要打发时间的休闲玩家，后者对应了有对战需求的竞技玩家。其各个模式都含有玩家的排行榜，鼓励玩家游玩。</p><h2 id="运营思路"><a href="#运营思路" class="headerlink" title="运营思路"></a>运营思路</h2><p>作为一款运营6年的游戏，微派网络的运营显然是成功的。但于此同时，观察微派网络官方的产品介绍，微派至今未能拿出较为重量的游戏产品…</p><p>贪吃蛇的运营思路：广告＋效果付费+礼物</p><p>对于免费玩家，通过看广告奖励游戏道具创收。对于付费玩家，通过游戏皮肤，击杀效果等场上道具，以及社群属性的赠送礼物，以及诸多例如show值，魅力值等系统排行展示，诱导消费。</p><h2 id="游戏弊端"><a href="#游戏弊端" class="headerlink" title="游戏弊端"></a>游戏弊端</h2><p>游戏的弊端大多存在于PvP模式下。</p><ul><li><p>网络平衡和链接稳定性差：网络稳定性是作为IO游戏能愉快游玩的关键因素，在不同网络条件下，同一局能一起愉快游玩必须做好网络平衡，否则对部分玩家，体验极差。然而贪吃蛇大作战的解决方案既不是增加链接更近的服务器，降低网络延迟。也不是交出满意的游戏网络平衡方案。而是通过减少网络质量差玩家的加速按钮使用来控制。不止到读者是否遇到加速按钮按不动没有效果的情况，极其影响竞技体验。（补充：同样的网络环境跑腾讯系游戏40ms EA平台94ms 但在贪吃蛇游戏中要么140+ 要么就是绿色信号的但卡按钮）</p></li><li><p>游戏皮肤平衡未完善；游戏皮肤不应该成为影响竞技体验的要素，但经过较长时间的游玩发现，不同皮肤的头部碰撞箱判定是不同的，导致对无皮肤玩家的不公平。</p></li><li><p>游戏判定问题：完全正碰无法计算。随机给一方？</p></li><li><p>游戏机型优化：苹果系设备在promotion高刷支持下游戏对局流畅度明显高于安卓机型，此外在PvP模式下此类特殊优化体验甚至高于网络优化令人唏嘘。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贪吃蛇大作战 </tag>
            
            <tag> 游戏分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络基础模型学习资源</title>
      <link href="/2022/12/08/%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8/"/>
      <url>/2022/12/08/%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络基础模型学习资源"><a href="#神经网络基础模型学习资源" class="headerlink" title="神经网络基础模型学习资源"></a>神经网络基础模型学习资源</h1><h2 id="视频课"><a href="#视频课" class="headerlink" title="视频课"></a>视频课</h2><ul><li><p><a href="https://space.bilibili.com/18161609/"><strong>霹雳吧啦Wz基础教程</strong></a></p></li><li><p><a href="https://space.bilibili.com/21241234"><strong>刘二大人</strong></a></p></li></ul><h2 id="博客笔记"><a href="#博客笔记" class="headerlink" title="博客笔记"></a>博客笔记</h2><ul><li><p><a href="https://blog.csdn.net/m0_37867091?type=blog"><strong>CSDN神经网络</strong></a></p></li><li><p><a href="https://redstonewill.com/category/ai-notes/"><strong>红色石头</strong></a></p></li><li><p><a href="http://www.ai-start.com/"><strong>吴恩达笔记</strong></a></p></li></ul><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>部分摘自<a href="https://cloud.tencent.com/developer/article/1882674">数据studio</a>，部分自行收集</p><h3 id="1、PlotNeuralNet"><a href="#1、PlotNeuralNet" class="headerlink" title="1、PlotNeuralNet"></a><strong>1、PlotNeuralNet</strong></h3><p>使用<strong>Latex</strong>绘制神经网络。传送门：<a href="https://github.com/HarisIqbal88/PlotNeuralNet">https://github.com/HarisIqbal88/PlotNeuralNet</a></p><p><img src="https://s2.loli.net/2022/12/10/zCPAx53wpQiryVj.png"></p><p>FCN-8模型</p><p>overleaf上Latex代码：<a href="https://www.overleaf.com/read/kkqntfxnvbsk">https://www.overleaf.com/read/kkqntfxnvbsk</a></p><p><img src="https://s2.loli.net/2022/12/10/Qy4pCEc7GMR6jnP.png"></p><p>FCN-32模型</p><p>overleaf上Latex代码：<a href="https://www.overleaf.com/read/wsxpmkqvjnbs">https://www.overleaf.com/read/wsxpmkqvjnbs</a></p><p><img src="https://s2.loli.net/2022/12/10/Izuf4rYX8WbwaCj.png"></p><p>Holistically-Nested Edge Detection</p><p>overleaf上Latex代码：<a href="https://www.overleaf.com/read/jxhnkcnwhfxp">https://www.overleaf.com/read/jxhnkcnwhfxp</a></p><hr><h3 id="2、Matlab"><a href="#2、Matlab" class="headerlink" title="2、Matlab"></a><strong>2、Matlab</strong></h3><p><a href="https://www.mathworks.com/help/deeplearning/ref/view.html;jsessionid=bd77484ba149c98d4d410abed983">https://www.mathworks.com/help/deeplearning/ref/view.html;jsessionid=bd77484ba149c98d4d410abed983</a></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[x,t] = iris_dataset;</span><br><span class="line">net = patternnet;</span><br><span class="line">net = <span class="title function_">configure</span>(net,x,t);</span><br><span class="line"><span class="title function_">view</span>(net)</span><br></pre></td></tr></table></figure><p>复制</p><p><img src="https://s2.loli.net/2022/12/10/y1eJVsHdMI2mDUA.png"></p><hr><h3 id="3、NN-SVG"><a href="#3、NN-SVG" class="headerlink" title="3、NN-SVG"></a><strong>3、NN-SVG</strong></h3><p>一个在线工具，点点就阔以了：<a href="http://alexlenail.me/NN-SVG/LeNet.html">http://alexlenail.me/NN-SVG/LeNet.html</a></p><p><img src="https://s2.loli.net/2022/12/10/8Xfd1VzRA3iI5gG.png"></p><p>AlexNet模型</p><p><img src="https://s2.loli.net/2022/12/10/A5TNwfOHscE4yBr.png"></p><p>LeNet模型</p><hr><h3 id="4、graphcore"><a href="#4、graphcore" class="headerlink" title="4、graphcore"></a><strong>4、graphcore</strong></h3><p>回到神经网络最初的地方，<strong>像生物细胞神经元neurons一样展示神经网络</strong>。<a href="https://www.graphcore.ai/posts/what-does-machine-learning-look-like">https://www.graphcore.ai/posts/what-does-machine-learning-look-like</a></p><p><img src="https://s2.loli.net/2022/12/10/t4LcPpaTgr7doCk.png"></p><p>生物细胞神经元模式图</p><p>AlexNet模型</p><p><img src="https://s2.loli.net/2022/12/10/MFJdwH2nvSX3RIb.png"></p><p>Resnet 50模型</p><hr><h3 id="5、graphviz"><a href="#5、graphviz" class="headerlink" title="5、graphviz"></a><strong>5、graphviz</strong></h3><p><a href="http://www.graphviz.org/">http://www.graphviz.org/</a></p><p>之前介绍过一个类似绘制网络关系的工具👉<a href="https://mp.weixin.qq.com/s?__biz=MzUwOTg0MjczNw==&mid=2247492450&idx=1&sn=045f3fad7573bef525c972b4e91e76c3&chksm=f90ea73cce792e2a92430c0eff73415380efce970f2641814f321fc2260a70ea686369925f3c&token=1079425073&lang=zh_CN&scene=21#wechat_redirect"><strong>盘一盘社交网络分析常用networks</strong></a></p><p><img src="https://s2.loli.net/2022/12/10/GZtAEw8HQUjRNeC.png"></p><p>4层网络</p><hr><h3 id="6、Keras"><a href="#6、Keras" class="headerlink" title="6、Keras"></a><strong>6、Keras</strong></h3><p><strong>深度学习框架Keras</strong>下的一个小模块，</p><p><a href="https://keras.io/api/utils/model_plotting_utils/">https://keras.io/api/utils/model_plotting_utils/</a></p><p><img src="https://s2.loli.net/2022/12/10/u9INipoZPFWMv37.png"></p><hr><h3 id="7、neataptic"><a href="#7、neataptic" class="headerlink" title="7、neataptic"></a><strong>7、neataptic</strong></h3><p><a href="https://github.com/wagenaartje/neataptic">https://github.com/wagenaartje/neataptic</a></p><p><img src="https://s2.loli.net/2022/12/10/M4s7r1JSgIKnFVf.png"></p><hr><h3 id="8、Quiver"><a href="#8、Quiver" class="headerlink" title="8、Quiver"></a><strong>8、Quiver</strong></h3><p><a href="https://github.com/keplr-io/quiver">https://github.com/keplr-io/quiver</a></p><p><img src="https://s2.loli.net/2022/12/10/klFsLSt7u4OfiXP.png"></p><hr><h3 id="9、Keras-js"><a href="#9、Keras-js" class="headerlink" title="9、Keras.js"></a><strong>9、Keras.js</strong></h3><p>在线工具</p><p><a href="https://transcranial.github.io/keras-js/#/inception-v3">https://transcranial.github.io/keras-js/#/inception-v3</a></p><p><img src="https://s2.loli.net/2022/12/10/lDbWQ4d7kZsVoRe.png"></p><p><img src="https://s2.loli.net/2022/12/10/XuzvrIKjty7lF6E.png"></p><hr><h3 id="10、Netscope-CNN-Analyzer"><a href="#10、Netscope-CNN-Analyzer" class="headerlink" title="10、Netscope CNN Analyzer"></a><strong>10、Netscope CNN Analyzer</strong></h3><p><a href="http://dgschwend.github.io/netscope/quickstart.html">http://dgschwend.github.io/netscope/quickstart.html</a></p><p><img src="https://s2.loli.net/2022/12/10/l2tK6oys9UZgOan.png"></p><hr><h3 id="11、keras-sequential-ascii"><a href="#11、keras-sequential-ascii" class="headerlink" title="11、keras-sequential-ascii"></a><strong>11、keras-sequential-ascii</strong></h3><p><a href="https://github.com/stared/keras-sequential-ascii/">https://github.com/stared/keras-sequential-ascii/</a></p><p>VGG 16 Architecture</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">     <span class="variable constant_">OPERATION</span>           <span class="variable constant_">DATA</span> <span class="variable constant_">DIMENSIONS</span>   <span class="title function_">WEIGHTS</span>(N)   <span class="title function_">WEIGHTS</span>(%)</span><br><span class="line"></span><br><span class="line">        <span class="title class_">Input</span>   #####      <span class="number">3</span>  <span class="number">224</span>  <span class="number">224</span></span><br><span class="line">   <span class="title class_">InputLayer</span>     |   -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####      <span class="number">3</span>  <span class="number">224</span>  <span class="number">224</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------      <span class="number">1792</span>     <span class="number">0.0</span>%</span><br><span class="line">         relu   #####     <span class="number">64</span>  <span class="number">224</span>  <span class="number">224</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------     <span class="number">36928</span>     <span class="number">0.0</span>%</span><br><span class="line">         relu   #####     <span class="number">64</span>  <span class="number">224</span>  <span class="number">224</span></span><br><span class="line"> <span class="title class_">MaxPooling2D</span>   Y max -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####     <span class="number">64</span>  <span class="number">112</span>  <span class="number">112</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------     <span class="number">73856</span>     <span class="number">0.1</span>%</span><br><span class="line">         relu   #####    <span class="number">128</span>  <span class="number">112</span>  <span class="number">112</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------    <span class="number">147584</span>     <span class="number">0.1</span>%</span><br><span class="line">         relu   #####    <span class="number">128</span>  <span class="number">112</span>  <span class="number">112</span></span><br><span class="line"> <span class="title class_">MaxPooling2D</span>   Y max -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####    <span class="number">128</span>   <span class="number">56</span>   <span class="number">56</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------    <span class="number">295168</span>     <span class="number">0.2</span>%</span><br><span class="line">         relu   #####    <span class="number">256</span>   <span class="number">56</span>   <span class="number">56</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------    <span class="number">590080</span>     <span class="number">0.4</span>%</span><br><span class="line">         relu   #####    <span class="number">256</span>   <span class="number">56</span>   <span class="number">56</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------    <span class="number">590080</span>     <span class="number">0.4</span>%</span><br><span class="line">         relu   #####    <span class="number">256</span>   <span class="number">56</span>   <span class="number">56</span></span><br><span class="line"> <span class="title class_">MaxPooling2D</span>   Y max -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####    <span class="number">256</span>   <span class="number">28</span>   <span class="number">28</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">1180160</span>     <span class="number">0.9</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">28</span>   <span class="number">28</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">2359808</span>     <span class="number">1.7</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">28</span>   <span class="number">28</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">2359808</span>     <span class="number">1.7</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">28</span>   <span class="number">28</span></span><br><span class="line"> <span class="title class_">MaxPooling2D</span>   Y max -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####    <span class="number">512</span>   <span class="number">14</span>   <span class="number">14</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">2359808</span>     <span class="number">1.7</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">14</span>   <span class="number">14</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">2359808</span>     <span class="number">1.7</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">14</span>   <span class="number">14</span></span><br><span class="line"><span class="title class_">Convolution2D</span>    \|/  -------------------   <span class="number">2359808</span>     <span class="number">1.7</span>%</span><br><span class="line">         relu   #####    <span class="number">512</span>   <span class="number">14</span>   <span class="number">14</span></span><br><span class="line"> <span class="title class_">MaxPooling2D</span>   Y max -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####    <span class="number">512</span>    <span class="number">7</span>    <span class="number">7</span></span><br><span class="line">      <span class="title class_">Flatten</span>   ||||| -------------------         <span class="number">0</span>     <span class="number">0.0</span>%</span><br><span class="line">                #####       <span class="number">25088</span></span><br><span class="line">        <span class="title class_">Dense</span>   <span class="variable constant_">XXXXX</span> ------------------- <span class="number">102764544</span>    <span class="number">74.3</span>%</span><br><span class="line">         relu   #####        <span class="number">4096</span></span><br><span class="line">        <span class="title class_">Dense</span>   <span class="variable constant_">XXXXX</span> -------------------  <span class="number">16781312</span>    <span class="number">12.1</span>%</span><br><span class="line">         relu   #####        <span class="number">4096</span></span><br><span class="line">        <span class="title class_">Dense</span>   <span class="variable constant_">XXXXX</span> -------------------   <span class="number">4097000</span>     <span class="number">3.0</span>%</span><br><span class="line">      softmax   #####        <span class="number">1000</span></span><br></pre></td></tr></table></figure><p>复制</p><hr><h3 id="12、TensorBoard"><a href="#12、TensorBoard" class="headerlink" title="12、TensorBoard"></a><strong>12、TensorBoard</strong></h3><p>一个评估<strong>深度学习框架TensorFlow</strong>模型的强力工具。</p><p><a href="https://www.tensorflow.org/tensorboard/graphs">https://www.tensorflow.org/tensorboard/graphs</a></p><p><img src="https://s2.loli.net/2022/12/10/2t4CA6bsEfDwJrP.png"></p><hr><h3 id="13、Caffe"><a href="#13、Caffe" class="headerlink" title="13、Caffe"></a><strong>13、Caffe</strong></h3><p>同样是<strong>深度学习框架Caffe</strong>下的一个小工具，</p><p><a href="https://github.com/BVLC/caffe/blob/master/python/caffe/draw.py">https://github.com/BVLC/caffe/blob/master/python/caffe/draw.py</a></p><p><img src="https://s2.loli.net/2022/12/10/FCp5ri67XHos8kd.png"></p><hr><h3 id="14、TensorSpace"><a href="#14、TensorSpace" class="headerlink" title="14、TensorSpace"></a><strong>14、TensorSpace</strong></h3><p><strong>3D模式展示神经网络</strong>，</p><p><a href="https://tensorspace.org/">https://tensorspace.org/</a></p><p><img src="https://s2.loli.net/2022/12/10/sH7plYeB3qMA982.png"></p><hr><h3 id="15、CNN-Explainer"><a href="#15、CNN-Explainer" class="headerlink" title="15、CNN Explainer"></a><strong>15、CNN Explainer</strong></h3><p>CNN解释器是 CNN可视化的工具，对于小白而言，CNN可视化对于理解CNN有非常的帮助，因此，花了几天的时间，将CNN解释器网站做了一个翻译，还包括安装CNN解释器的过程和相关资料。<br>CNN解释器地址：<a href="https://poloclub.github.io/cnn-explainer">CNN Explainer</a></p><p>CNN解释器文献：<a href="https://arxiv.org/abs/2004.15004">CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization</a></p><p>CNN github地址：<a href="https://github.com/poloclub/cnn-explainer">https://github.com/poloclub/cnn-explainer</a></p><p>CNN解释器安装：<a href="https://zhuanlan.zhihu.com/p/141537738">https://zhuanlan.zhihu.com/p/141537738</a> </p><p><img src="https://s2.loli.net/2022/12/10/IXrZRDbckK71hW6.png"></p><h3 id="16、基本操作-人工智能的诞生"><a href="#16、基本操作-人工智能的诞生" class="headerlink" title="16、基本操作-人工智能的诞生"></a><strong>16、基本操作-人工智能的诞生</strong></h3><p>交互视频课程，极其适合初学者，就是有点贵，运营歪屁股<br><a href="https://jibencaozuo.com/zh-Hans/videoSeries/1/episode/0">人工智能的诞生</a></p><p><img src="https://s2.loli.net/2022/12/10/MsajcZUqnVoYAHd.png"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 学习资源 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN基础</title>
      <link href="/2022/12/08/CNN%E5%9F%BA%E7%A1%80/"/>
      <url>/2022/12/08/CNN%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="CNN基础"><a href="#CNN基础" class="headerlink" title="CNN基础"></a>CNN基础</h1><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><ol><li><p><strong>图像分类（Image Classification）</strong></p></li><li><p><strong>目标识别（Object detection）</strong></p></li><li><p><strong>神经风格转换（Neural Style Transfer）</strong></p></li></ol><p><strong>使用传统神经网络处理机器视觉的一个主要问题是输入层维度很大</strong>。如果图片尺寸较大，例如一张1000x1000x3的图片，神经网络输入层的维度将达到3百万，使得<strong>网络权重W非常庞大</strong>。这样会造成两个后果，一是神经网络结构复杂，数据量相对不够，容易出现过拟合；二是所需内存、计算量较大。<strong>解决这一问题的方法就是使用卷积神经网络（CNN）。</strong></p><p><strong>CNN做的事情其实是，来简化这个neural network的架构，我们根据自己的知识和对图像处理的理解，一开始就把某些实际上用不到的参数给过滤掉</strong>，我们一开始就想一些办法，不要用fully connected network，而是用比较少的参数，来做图像处理这件事情，所以CNN其实是比一般的DNN还要更简单的。</p><hr><h2 id="卷积操作：以边缘检测举例"><a href="#卷积操作：以边缘检测举例" class="headerlink" title="卷积操作：以边缘检测举例"></a>卷积操作：以边缘检测举例</h2><p>（Edge Detection）</p><p>图片边缘检测的方式</p><ol><li>垂直边缘检测（Vertical edges）</li><li>水平边缘检测（Horizontal edges）</li></ol><p><img src="https://s2.loli.net/2022/12/07/ADTrBFMpjeycq4l.png"></p><p>边缘检测通过相应的滤波器（卷积核）卷积实现。</p><h3 id="【示例】垂直检测"><a href="#【示例】垂直检测" class="headerlink" title="【示例】垂直检测"></a>【示例】垂直检测</h3><p><img src="http://www.ai-start.com/dl2017/images/9aa008335e8a229d3818a61aaccc7173.png" alt="垂直检测1"></p><p><img src="https://img-blog.csdnimg.cn/2020042210323963.png#pic_center" alt="垂直检测2"></p><p>图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为6×6，滤波器filter尺寸为3×3，卷积后的图片尺寸为4×4</p><p>其中 *****代表卷积 上图只显示了卷积后的第一个值和最后一个值，其余值可自行计算。</p><h2 id="边缘检测补充"><a href="#边缘检测补充" class="headerlink" title="边缘检测补充"></a>边缘检测补充</h2><p>图像边缘有两种渐变方式，一种是由明变暗，另一种是由暗变明。以垂直边缘检测为例，下图展示了两种方式的区别。实际应用中，这两种渐变方式并不影响边缘检测结果，可以对输出图像取绝对值操作，得到同样的结果。</p><p><img src="https://img-blog.csdnimg.cn/20200422104421714.png#pic_center" alt="在这里插入图片描述"><br>下图展示一个水平边缘检测的例子：</p><p><img src="https://img-blog.csdnimg.cn/20200422104724389.png#pic_center" alt="在这里插入图片描述"></p><p>垂直边缘检测和水平边缘检测的滤波器<a href="https://so.csdn.net/so/search?q=%E7%AE%97%E5%AD%90&spm=1001.2101.3001.7020">算子</a>如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20200422104617695.png#pic_center" alt="在这里插入图片描述"><br>除了上面提到的这种简单的Vertical、Horizontal滤波器之外，还有其它常用的filters，例如Sobel filter和Scharr filter。这两种滤波器的特点是增加图片中心区域的权重。（下图展示的是垂直边缘检测算子，水平边缘检测算子只需将上图顺时针翻转90度即可。）</p><p><img src="https://img-blog.csdnimg.cn/20200422105237527.png#pic_center" alt="在这里插入图片描述"><br>在深度学习中，如果我们想检测图像的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么 filter 的数值一般需要通过模型训练得到，类似于标准神经网络中的权重 w ww 一样由反向传播算法迭代求得。CNN的主要目的就是计算出这些 filter 的数值。确定得到了这些 filter 后，CNN浅层网络也就实现了对图片所有边缘特征的检测。<br><img src="https://img-blog.csdnimg.cn/2020042210550658.png#pic_center" alt="在这里插入图片描述"></p><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>按照我们上面讲的图片卷积，如果原始图片尺寸为 n × n，filter尺寸为 f × f，则卷积后的图片尺寸为 ( n − f + 1 ) × ( n − f + 1 ) ，注意 f 一般为奇数。这样会带来两个问题：</p><ol><li>卷积运算后，输出图片尺寸缩小</li><li>原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息</li></ol><p>为了解决图片缩小的问题，可以 <strong>使用padding方法，即把原始图片尺寸进行扩展，扩展区域补零</strong>，用 p pp 来表示每个方向扩展的宽度。<br><img src="https://img-blog.csdnimg.cn/20200422110159369.png#pic_center" alt="在这里插入图片描述"><br>经过padding之后:</p><table><thead><tr><th>原始图像padding后尺寸</th><th>filter尺寸</th><th>卷积后的图像尺寸</th></tr></thead><tbody><tr><td>( n + 2 p ) × ( n + 2 p )</td><td>f × f</td><td>( n + 2 p − f + 1 ) × ( n + 2 p − f + 1 )</td></tr></tbody></table><p>稍作总结：</p><ul><li>无padding操作，p &#x3D; 0，我们称之为 <strong>Valid convolutions</strong> （不填充）</li><li>有padding操作，$\ p&#x3D;\frac{f-1}{2}$我们称之为 <strong>Same convolutions</strong> （填充，输入输出大小相等）</li></ul><hr><h2 id="卷积步长"><a href="#卷积步长" class="headerlink" title="卷积步长"></a>卷积步长</h2><p><img src="https://img-blog.csdnimg.cn/20200422111528803.png#pic_center" alt="卷积步长"></p><p>我们用s 表示stride长度，p 表示padding长度，如果原始图片尺寸为 n × n，filter（卷积核）尺寸为 f × f ，则卷积后的图片尺寸为：</p><p><img src="https://s2.loli.net/2022/12/07/HwfI68tMDunColL.png"></p><p>注：商不是整数的情况下，向下取整</p><h3 id="数学上卷积与人工智能卷积的区别："><a href="#数学上卷积与人工智能卷积的区别：" class="headerlink" title="数学上卷积与人工智能卷积的区别："></a>数学上卷积与人工智能卷积的区别：</h3><ul><li>数学意义上的卷积（<strong>convolutions</strong>））运算会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。filter旋转如下所示：<br><img src="https://img-blog.csdnimg.cn/20200422112729414.png#pic_center" alt="在这里插入图片描述"></li><li>人工智能意义上卷积：相关系数（<strong>cross-correlations</strong>）的计算过程则不会对filter进行旋转，而是直接在原始图片上进行滑动计算。</li></ul><p><strong>总的来说，忽略旋转运算可以大大提高CNN网络运算速度，而且不影响模型性能。</strong></p><p>注：卷积运算服从结合率不服从交换律</p><hr><h2 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h2><p>同二维卷积多了一个维度</p><ul><li><p>卷积核 channel（深度&#x2F;通道数）与输入特征层 的channel 相同</p></li><li><p>输出的特征矩阵channel与卷积核个数相同</p></li></ul><p>（Convolutions over volumes）</p><p>对于3通道的RGB图像，其对应的滤波器算子同样也是3通道的。例如一个图像是6 x 6 x 3，分别表示图像的高度（height）、宽度（weight）和通道（channel）。</p><p>3通道图像的卷积运算与单通道图像的卷积运算基本一致。过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再<strong>将3通道的和相加</strong>，得到输出图像的<strong>一个</strong>像素值。</p><p><img src="https://img-blog.csdnimg.cn/20200422113610810.png#pic_center" alt="在这里插入图片描述"><br>不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。</p><p>为了实现更多边缘检测，可以增加更多的滤波器组，进行多个卷积运算。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p><p><img src="https://img-blog.csdnimg.cn/20200422113807669.png#pic_center" alt="在这里插入图片描述"></p><p><img src="https://s2.loli.net/2022/12/07/sBoEdSWhqKINkc4.png"></p><h2 id="单层卷积网络"><a href="#单层卷积网络" class="headerlink" title="单层卷积网络"></a>单层卷积网络</h2><p>卷积神经网络的单层结构如下所示：<br><img src="https://img-blog.csdnimg.cn/20200422151012841.png#pic_center" alt="卷积神经网络的单层结构如下所示："></p><p><img src="https://s2.loli.net/2022/12/07/BkVoKx69ZDuY8PF.png"></p><p>总结</p><p><img src="https://s2.loli.net/2022/12/07/tXxq1EfkgDBvYeT.png"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
